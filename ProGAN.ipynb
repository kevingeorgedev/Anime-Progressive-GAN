{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"C:\\.Python Projects\\AnimeGAN\\data\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 14\n",
    "\n",
    "# Set True to prevent releasing and reassigning workers between epochs\n",
    "persistent_workers = True\n",
    "\n",
    "# Number of batches to prefetch per worker ( workers * prefect_factor = Number of Batches preloaded )\n",
    "prefetch_factor = 4\n",
    "\n",
    "# Batch size during training\n",
    "batch_sizes = [64, 64, 64, 64, 40, 16, 8, 4] # 512: [64, 64, 64, 64, 10, 6, 4, 2], 256: [64, 64, 64, 64, 32, 16, 8, 4]\n",
    "\n",
    "# Image size to train up to (inclusive). Paper uses 1024\n",
    "image_size = 256\n",
    "\n",
    "assert image_size >= 4, f\"{image_size} is not greater than or equal to 4!\"\n",
    "assert image_size <= 1024, f\"{image_size} is not less than or equal to 1024!\"\n",
    "assert math.ceil(log2(image_size)) == math.floor(log2(image_size)), f\"{image_size} is not a power of 2!\"\n",
    "\n",
    "# Image size to start training at\n",
    "start_train_at = 4\n",
    "\n",
    "assert image_size >= 4, f\"{start_train_at} is not greater than or equal to 4!\"\n",
    "assert start_train_at <= image_size, f\"{start_train_at} is not less than or equal to {image_size}\"\n",
    "assert math.ceil(log2(start_train_at)) == math.floor(log2(start_train_at)), f\"{start_train_at} is not a power of 2!\"\n",
    "\n",
    "# Device to push to\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "pin_memory = True if device.type == \"cuda\" else False\n",
    "\n",
    "# Size of latent vector, Paper uses 512 for z_dim and in_channels\n",
    "z_dim = 256\n",
    "\n",
    "in_channels = 256\n",
    "\n",
    "# Learning rate for optimizers. Paper uses 1e-3\n",
    "lr = 1e-3\n",
    "\n",
    "lambda_gp = 10\n",
    "\n",
    "# Number of steps to reach desired image size\n",
    "num_steps = int(log2(image_size / 4)) + 1\n",
    "\n",
    "# Progressive Epochs\n",
    "#   4x4: Train the model to see 800k images. \n",
    "#   8x8, 16x16, ..., img_size x img_size: Train model to see 800k images fading in the new layer and 800k images to stabilize.\n",
    "#   \n",
    "#   4x4 epochs: 800,000 / dataset_size\n",
    "#   onwards... 2 times 4x4 epochs\n",
    "\n",
    "prog_epochs = [32] + [64] * (num_steps - 1)\n",
    "\n",
    "# Used to create the layers progressively.\n",
    "factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n",
    "\n",
    "# Fixed noise to monitor progression of model\n",
    "fixed_noise = torch.randn(64, z_dim, 1, 1).to(device)\n",
    "\n",
    "# Display images after each epoch\n",
    "display_images = False\n",
    "\n",
    "# Use pretrained model\n",
    "use_pretrained = True\n",
    "\n",
    "# Set true to show every step with tqdm\n",
    "update_last = True\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "step = int(log2(start_train_at / 4))\n",
    "\n",
    "if use_pretrained:\n",
    "    path = \"models\\pretrained_imgsize_128_zdim_256.pth\" # \"../PATH_TO_CHECKPOINT.pth\"\n",
    "    checkpoint = torch.load(path)\n",
    "    batch_sizes = checkpoint[\"batch_sizes\"]\n",
    "    start_train_at = checkpoint[\"start_training_at\"]\n",
    "    fixed_noise = checkpoint[\"fixed_noise\"]\n",
    "    z_dim = checkpoint[\"z_dim\"]\n",
    "    in_channels = checkpoint[\"in_channels\"]\n",
    "    step = int(log2(start_train_at / 4))\n",
    "    start_epoch = checkpoint[\"epoch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted-Scaled Convolutional Layer\n",
    "\n",
    "Equalized Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self, input_channel, out_channel, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channel, out_channel, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (input_channel * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel Normalization\n",
    "\n",
    "Normalization replacement for batch normalization.\n",
    "\n",
    "Configured as $b_{x, y} = a_{x, y} / \\sqrt{\\frac{1}{N} \\sum_{j=0}^{N-1}(a_{x,y}^j)^2 + \\epsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt( torch.mean( x ** 2, dim=1, keepdim=True ) + self.epsilon )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_channel, out_channel, pixel_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(input_channel, out_channel)\n",
    "        self.conv2 = WSConv2d(out_channel, out_channel)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pn = pixel_norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0), # 1x1 -> 4x4\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm()\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([self.initial_rgb])\n",
    "\n",
    "        for i in range(len(factors) - 1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i+1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial(x)\n",
    "        \n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "        \n",
    "        for i in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode=\"nearest\")\n",
    "            out = self.prog_blocks[i](upscaled)\n",
    "        \n",
    "        final_upscaled = self.rgb_layers[steps-1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i-1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, pixel_norm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "        # 4x4 img res\n",
    "        self.initial_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, 1, kernel_size=1, padding=0, stride=1),\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x: torch.Tensor):\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        \n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:  # i.e, image is 4x4\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim=z_dim, in_channels=in_channels)\n",
    "disc = Discriminator(in_channels=in_channels)\n",
    "\n",
    "for img_size in [4, 8, 16, 32, 64, 128, 256, 512]:\n",
    "    num_steps = int(log2(img_size / 4))\n",
    "    x = torch.randn((1, z_dim, 1, 1))\n",
    "    z = gen(x, 0.5, steps=num_steps)\n",
    "    assert z.shape == (1, 3, img_size, img_size)\n",
    "    out = disc(z, alpha=0.5, steps=num_steps)\n",
    "    print(f\"Success! at img size: {img_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # Resize to a standard size (can be adjusted)\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # Normalize to [-1, 1]\n",
    "    ])\n",
    "\n",
    "    batch_size = batch_sizes[int(log2(image_size / 4))]\n",
    "\n",
    "    dataset = dset.ImageFolder(root=dataroot, transform=transform)\n",
    "\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True, \n",
    "                            num_workers=workers, \n",
    "                            drop_last=True, \n",
    "                            pin_memory=pin_memory, \n",
    "                            persistent_workers=persistent_workers, \n",
    "                            prefetch_factor=prefetch_factor)\n",
    "\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(disc, gen, loader, dataset, step, alpha, opt_disc, opt_gen, epoch, num_epochs):\n",
    "    loop = tqdm(loader, total=len(loader), miniters=50, desc=f\"Epoch [{epoch + 1}/{num_epochs}]\", leave=update_last)\n",
    "    for real, _ in loop:\n",
    "        #if batch_idx == 10:\n",
    "        #    break\n",
    "\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        # Generate noise and fake data\n",
    "        noise = torch.randn(cur_batch_size, z_dim, 1, 1).to(device)\n",
    "        fake = gen(noise, alpha, step)\n",
    "\n",
    "        # Train Discriminator\n",
    "        disc_real = disc(real, alpha, step)\n",
    "        disc_fake = disc(fake.detach(), alpha, step)\n",
    "        gp = gradient_penalty(disc, real, fake, alpha, step, device)\n",
    "        loss_disc = (\n",
    "            -(torch.mean(disc_real) - torch.mean(disc_fake))\n",
    "            + lambda_gp * gp\n",
    "            + (0.001 * torch.mean(disc_real ** 2))\n",
    "        )\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generator\n",
    "        gen_fake = disc(fake, alpha, step)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Update alpha for progressive growing\n",
    "        alpha += cur_batch_size / ((prog_epochs[step] * 0.5) * len(dataset))\n",
    "\n",
    "        alpha = min(alpha, 1)\n",
    "    \n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Networks and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gen = Generator(z_dim=z_dim, in_channels=in_channels).to(device)\n",
    "disc = Discriminator(in_channels=in_channels).to(device)\n",
    "\n",
    "optimizer_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.0, 0.99))\n",
    "optimizer_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.0, 0.99))\n",
    "\n",
    "if use_pretrained:\n",
    "    gen.load_state_dict(checkpoint['gen_state'])\n",
    "    disc.load_state_dict(checkpoint['disc_state'])\n",
    "    optimizer_gen.load_state_dict(checkpoint['gen_optim'])\n",
    "    optimizer_disc.load_state_dict(checkpoint['disc_optim'])\n",
    "\n",
    "fakes = []\n",
    "\n",
    "if use_pretrained:\n",
    "    fakes = checkpoint[\"fakes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "change_alpha = use_pretrained\n",
    "\n",
    "for num_epochs in prog_epochs[step:]:\n",
    "    if start_train_at > image_size:\n",
    "        break\n",
    "    alpha = 1e-5\n",
    "    if change_alpha:\n",
    "        alpha = checkpoint['alpha']\n",
    "        change_alpha = False\n",
    "\n",
    "    loader, dataset = get_loader(4 * 2 ** step)\n",
    "    \n",
    "    print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "\n",
    "        alpha = train(\n",
    "            disc,\n",
    "            gen,\n",
    "            loader,\n",
    "            dataset,\n",
    "            step,\n",
    "            alpha,\n",
    "            optimizer_disc,\n",
    "            optimizer_gen,\n",
    "            epoch,\n",
    "            num_epochs,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img = gen(fixed_noise, alpha, step) * 0.5 + 0.5\n",
    "\n",
    "        fakes.append(img)\n",
    "\n",
    "        if display_images:\n",
    "            img = img.cpu().detach()\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "            # Display each image\n",
    "            fig, axes = plt.subplots(8, 8, figsize=(8, 8))  # Create a 8x8 grid\n",
    "            axes = axes.flatten()\n",
    "\n",
    "            for i, ax in enumerate(axes):\n",
    "                image = img[i].permute(1, 2, 0).numpy()  # Rearrange dimensions to (H, W, C)\n",
    "                ax.imshow(image)\n",
    "                ax.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if epoch + 1 != num_epochs:\n",
    "            torch.save({\n",
    "                'batch_sizes': batch_sizes,\n",
    "                'start_training_at': 4 * 2 ** step,\n",
    "                'alpha': alpha,\n",
    "                'fixed_noise': fixed_noise,\n",
    "                'z_dim': z_dim,\n",
    "                'in_channels': in_channels,\n",
    "                'epoch': epoch + 1,\n",
    "                'fakes': fakes,\n",
    "                'gen_state': gen.state_dict(),\n",
    "                'disc_state': disc.state_dict(),\n",
    "                'gen_optim': optimizer_gen.state_dict(),\n",
    "                'disc_optim': optimizer_disc.state_dict(),\n",
    "            }, f\"models/training_imgsize_{4 * 2 ** (step)}_zdim_{z_dim}_progression.pth\")\n",
    "\n",
    "    torch.save({\n",
    "        'batch_sizes': batch_sizes,\n",
    "        'start_training_at': (4 * 2 ** step) * 2,\n",
    "        'alpha': 1e-5,\n",
    "        'fixed_noise': fixed_noise,\n",
    "        'z_dim': z_dim,\n",
    "        'in_channels': in_channels,\n",
    "        'epoch': 0,\n",
    "        'fakes': fakes,\n",
    "        'gen_state': gen.state_dict(),\n",
    "        'disc_state': disc.state_dict(),\n",
    "        'gen_optim': optimizer_gen.state_dict(),\n",
    "        'disc_optim': optimizer_disc.state_dict(),\n",
    "    }, f\"models/pretrained_imgsize_{4 * 2 ** step}_zdim_{z_dim}.pth\")\n",
    "\n",
    "    step += 1\n",
    "\n",
    "gen.eval()\n",
    "disc.eval()\n",
    "\n",
    "print(\"Eval mode activated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create grid of sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "\n",
    "for x in [x.detach().cpu() for x in fakes]: # To shorten number of images: [x[:8] for x in fakes]\n",
    "    x = torch.nn.functional.interpolate(x, size=(128, 128), mode=\"nearest\")\n",
    "    img_list.append(vutils.make_grid(x, padding=2, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GIF displaying generation over training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "ani.save(\"anime.gif\", writer='pillow', fps=10)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Real vs. Fake Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "dataloader, dataset = get_loader(image_size=128)\n",
    "real_batch = torch.stack([dataset[i][0] for i in range(64)]).to(device)\n",
    "#real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch, padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.eval()\n",
    "disc.eval()\n",
    "print(\"Eval mode set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "noise = torch.randn(1, z_dim, 1, 1).to(device)\n",
    "\n",
    "output: torch.Tensor = gen(noise, 1, step - 1) * 0.5 + 0.5\n",
    "output = output.squeeze(0)\n",
    "img = to_pil_image(output)\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def interpolate_points(p1, p2, n_steps=100):\n",
    "    ratios = np.linspace(0, 1, num=n_steps)[1:]\n",
    "    vectors = [p1]\n",
    "    for ratio in ratios:\n",
    "        v = (1.0 - ratio) * p1 + ratio * p2\n",
    "        vectors.append(v)\n",
    "    return torch.stack(vectors, dim=0)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"point1 = torch.randn(z_dim, 1, 1)\n",
    "point2 = torch.randn(z_dim, 1, 1)\n",
    "\n",
    "points = interpolate_points(point1, point2, n_steps=100)\n",
    "\n",
    "images = [to_pil_image((gen(p.to(device), 1, step - 1) * 0.5 + 0.5).squeeze(0)) for p in points]\n",
    "images[0].save(\"progress.gif\", save_all=True, append_images=images[1:], duration=10, loop=2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "start_train_at = 4 * 2 ** step\n",
    "epoch = 0 # Specify the epoch to resume training at\n",
    "\n",
    "torch.save({\n",
    "    'batch_sizes': batch_sizes,\n",
    "    'start_training_at': start_train_at,\n",
    "    'fixed_noise': fixed_noise,\n",
    "    'z_dim': z_dim,\n",
    "    'in_channels': in_channels,\n",
    "    'factors': factors,\n",
    "    'epoch': epoch,\n",
    "    'fakes': fakes,\n",
    "    'gen_state': gen.state_dict(),\n",
    "    'disc_state': disc.state_dict(),\n",
    "    'gen_optim': optimizer_gen.state_dict(),\n",
    "    'disc_optim': optimizer_disc.state_dict(),\n",
    "}, f\"model_imgsize_{4 * 2 ** step}_continue_epoch_{epoch}.pth\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
